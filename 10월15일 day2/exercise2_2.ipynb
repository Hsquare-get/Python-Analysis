{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://media.daum.net/ranking/popular/\"\n",
    "html_doc = requests.get(url)\n",
    "html_doc.encoding = \"utf-8\"\n",
    "# print(html_doc.text)\n",
    "\n",
    "bs = BeautifulSoup(html_doc.text, 'html.parser') # html_doc는 바이트열로 응답받았지만 BeautifulSoup는 str 객체가 필요\n",
    "# print(bs)\n",
    "\n",
    "# 리스트로 반환\n",
    "newstitle_list = bs.select('#mArticle div.cont_thumb a') # print(len(newstitle_list))\n",
    "newscomname_list = bs.select('#mArticle div.cont_thumb span.info_news') # print(len(newscomname_list))\n",
    "\n",
    "news = []\n",
    "for newstitle, newscomname in zip(newstitle_list, newscomname_list):\n",
    "    news.append([newstitle.text, newscomname.text])\n",
    "\n",
    "# wt : (텍스트) 파일에 기록한다. 파일이 이미 있으면 덮어쓴다\n",
    "# newline 설정을 안하면 한줄마다 공백있는 줄이 생긴다\n",
    "# encoding 아규먼트를 주지않으면 ANSI 파일로 저장되고 한글이 깨지지 않는다\n",
    "with open('C:/PyStexam/10월15일 day2/news.csv', \"wt\", newline=\"\", encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['newstitle', 'newscomname'])\n",
    "    for i in range(len(news)):\n",
    "        writer.writerow(news[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydatavenv",
   "language": "python",
   "name": "pydatavenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
